import glob
import os
import re
import shutil
import sys
from pathlib import Path
from random import randint

import numpy as np
import pandas as pd
import s3fs
from omegaconf import OmegaConf

from marius.tools.configuration.constants import PathConstants
from marius.tools.configuration.marius_config import DatasetConfig
from marius.tools.preprocess.converters.spark_constants import (
    DST_EDGE_BUCKET_COL,
    INDEX_COL,
    REL_INDEX_COL,
    SRC_EDGE_BUCKET_COL,
    TMP_DATA_DIRECTORY,
)
from marius.tools.preprocess.utils import get_df_count


# TODO can this be made faster? Pandas is pretty slow and not parallel
def convert_to_binary(input_filename, output_filename):
    assert input_filename != output_filename
    with open(output_filename, "wb") as output_file:
        for chunk in pd.read_csv(input_filename, header=None, chunksize=10**8, sep="\t", dtype=int):
            chunk_array = chunk.to_numpy(dtype=np.int32)
            output_file.write(bytes(chunk_array))

    os.system("rm {}".format(input_filename))


# TODO we can make this faster by using the cat bash command to combine these files super fast
def merge_csvs(input_directory, output_file):
    all_csvs = []
    for filename in glob.iglob(input_directory + "/**/*.csv", recursive=True):
        all_csvs.append(filename)

    print("Merging CSVs from {} to {}".format(input_directory, output_file))
    os.system("rm -rf {}".format(output_file))
    for source_file in all_csvs:
        os.system("cat {} >> {}".format(source_file, output_file))

    os.system("rm -rf {}".format(input_directory))


def s3_write_df_to_csv(s3_obj, df, output_folder, output_filename):
    if df is None:
        return

    file_pattern = re.compile(r".*part-.*\.csv")
    df.write.csv(output_folder, mode="overwrite", sep="\t")
    files_list = [f for f in s3_obj.ls(output_folder) if file_pattern.match(f)]
    if len(files_list) < 2:
        # rename to csv
        # s3_obj.rename("s3a://{}".format(files_list[0]), output_filename)
        pass
    else:
        # CompleteMultipartUpload errors out saying EntityTooSmall when using merge
        # need to create a single file from files_list
        # s3_obj.merge(output_filename, files_list)
        # TODO: merge all node outputs into output_filename
        pass


def write_df_to_csv(df, output_filename):
    if df is None:
        return

    tmp_dir = TMP_DATA_DIRECTORY + str(randint(0, sys.maxsize))
    df.write.csv(tmp_dir, mode="overwrite", sep="\t")
    merge_csvs(tmp_dir, output_filename)


def s3_write_partitioned_df_to_csv(s3_obj, partition_triples, num_partitions, output_folder, output_filename):
    if partition_triples is None:
        return

    bucket_counts = partition_triples.groupBy([SRC_EDGE_BUCKET_COL, DST_EDGE_BUCKET_COL]).count()
    partition_triples.write.partitionBy([SRC_EDGE_BUCKET_COL, DST_EDGE_BUCKET_COL]).csv(
        output_folder, mode="overwrite", sep="\t"
    )
    bucket_counts.write.partitionBy(SRC_EDGE_BUCKET_COL).csv(output_folder + "_counts", mode="overwrite", sep="\t")

    partition_offsets = []
    for src_part in range(num_partitions):
        tmp_counts_files = s3_obj.glob("{}_counts/src_part={}/part-*.csv".format(output_folder, str(src_part)))
        edges_bucket_counts = np.zeros(num_partitions, dtype=np.int)
        for tmp_counts_file in tmp_counts_files:
            counts = pd.read_csv("s3a://{}".format(tmp_counts_file), sep="\t", header=None)
            dst_buckets = counts.iloc[:, 0].values
            dst_counts = counts.iloc[:, 1].values
            edges_bucket_counts[dst_buckets] = dst_counts
        partition_offsets.append(edges_bucket_counts)

    # TODO: merge all the csvs generated by partition_triples into output_filename

    return np.concatenate(partition_offsets)


def write_partitioned_df_to_csv(partition_triples, num_partitions, output_filename):
    if partition_triples is None:
        return

    bucket_counts = partition_triples.groupBy([SRC_EDGE_BUCKET_COL, DST_EDGE_BUCKET_COL]).count()

    print(partition_triples.rdd.getNumPartitions())

    partition_triples.write.partitionBy([SRC_EDGE_BUCKET_COL, DST_EDGE_BUCKET_COL]).csv(
        TMP_DATA_DIRECTORY + "_edges", mode="overwrite", sep="\t"
    )

    bucket_counts.write.partitionBy(SRC_EDGE_BUCKET_COL).csv(TMP_DATA_DIRECTORY + "_counts", mode="overwrite", sep="\t")

    partition_offsets = []

    os.system("rm -rf {}".format(output_filename))
    for src_part in range(num_partitions):
        for dst_part in range(num_partitions):
            tmp_edges_files = glob.glob(
                "{}/{}={}/{}={}/*.csv".format(
                    TMP_DATA_DIRECTORY + "_edges",
                    SRC_EDGE_BUCKET_COL,
                    str(src_part),
                    DST_EDGE_BUCKET_COL,
                    str(dst_part),
                )
            )

            for tmp_edges_file in tmp_edges_files:
                os.system("cat {} >> {}".format(tmp_edges_file, output_filename))

        tmp_counts_files = glob.glob(
            "{}/{}={}/*.csv".format(TMP_DATA_DIRECTORY + "_counts", SRC_EDGE_BUCKET_COL, str(src_part))
        )

        edges_bucket_counts = np.zeros(num_partitions, dtype=np.int)
        for tmp_counts_file in tmp_counts_files:
            counts = pd.read_csv(tmp_counts_file, sep="\t", header=None)

            dst_buckets = counts.iloc[:, 0].values
            dst_counts = counts.iloc[:, 1].values

            edges_bucket_counts[dst_buckets] = dst_counts

        partition_offsets.append(edges_bucket_counts)

    os.system("rm -rf {}".format(TMP_DATA_DIRECTORY + "_edges"))
    os.system("rm -rf {}".format(TMP_DATA_DIRECTORY + "_counts"))

    return np.concatenate(partition_offsets)


class SparkWriteBase(object):
    def __init__(
        self,
        spark,
        output_dir,
        partitioned_evaluation,
        train_edges_df,
        valid_edges_df,
        test_edges_df,
        nodes_df,
        rels_df,
        num_partitions,
    ):
        self.spark = spark
        self.output_dir = output_dir
        self.partitioned_evaluation = partitioned_evaluation
        self.train_edges_df = train_edges_df
        self.valid_edges_df = valid_edges_df
        self.test_edges_df = test_edges_df
        self.nodes_df = nodes_df
        self.rels_df = rels_df
        self.num_partitions = num_partitions

    def get_dataset_stats(self):
        dataset_stats = DatasetConfig()
        dataset_stats.num_edges = self.train_edges_df.count()
        dataset_stats.num_train = dataset_stats.num_edges

        if self.valid_edges_df is not None:
            dataset_stats.num_valid = self.valid_edges_df.count()
        if self.test_edges_df is not None:
            dataset_stats.num_test = self.test_edges_df.count()

        dataset_stats.num_nodes = get_df_count(self.nodes_df, INDEX_COL)

        if self.rels_df is None:
            dataset_stats.num_relations = 1
        else:
            dataset_stats.num_relations = get_df_count(self.rels_df, REL_INDEX_COL)

        return dataset_stats


class SparkWriterLocal(SparkWriteBase):
    def __init__(
        self,
        spark,
        output_dir,
        partitioned_evaluation,
        train_edges_df,
        valid_edges_df,
        test_edges_df,
        nodes_df,
        rels_df,
        num_partitions,
    ):
        super().__init__(
            spark,
            output_dir,
            partitioned_evaluation,
            train_edges_df,
            valid_edges_df,
            test_edges_df,
            nodes_df,
            rels_df,
            num_partitions,
        )
        self.edges_helper = {
            "train": [self.train_edges_df, self.output_dir / Path(PathConstants.train_edges_path)],
            "valid": [self.valid_edges_df, self.output_dir / Path(PathConstants.valid_edges_path)],
            "test": [self.test_edges_df, self.output_dir / Path(PathConstants.test_edges_path)],
        }
        self.offset_helper = {
            "train": self.output_dir / Path(PathConstants.train_edge_buckets_path),
            "valid": self.output_dir / Path(PathConstants.valid_edge_buckets_path),
            "test": self.output_dir / Path(PathConstants.test_edge_buckets_path),
        }

    def write_dataset_stats(self):
        dataset_stats = self.get_dataset_stats()
        dataset_stats.dataset_dir = Path(self.output_dir).absolute().__str__()
        with open(self.output_dir / Path("dataset.yaml"), "w") as f:
            print("Dataset statistics written to: {}".format((self.output_dir / Path("dataset.yaml")).__str__()))
            yaml_file = OmegaConf.to_yaml(dataset_stats)
            f.writelines(yaml_file)
        return dataset_stats

    def write_nodes_df(self):
        write_df_to_csv(self.nodes_df, self.output_dir / Path(PathConstants.node_mapping_path))

    def write_rels_df(self):
        if self.rels_df is not None:
            write_df_to_csv(self.rels_df, self.output_dir / Path(PathConstants.relation_mapping_path))

    def write_partitioned_df_to_csv(self, mode):
        return write_partitioned_df_to_csv(self.edges_helper[mode][0], self.num_partitions, self.edges_helper[mode][1])

    def write_edge_offsets(self, mode, offsets):
        with open(self.offset_helper[mode], "w") as f:
            f.writelines([str(o) + "\n" for o in offsets])

    def write_edges_df(self, mode):
        write_df_to_csv(self.edges_helper[mode][0], self.edges_helper[mode][1])


class SparkWriteS3(SparkWriteBase):
    def __init__(
        self,
        spark,
        output_dir,
        partitioned_evaluation,
        train_edges_df,
        valid_edges_df,
        test_edges_df,
        nodes_df,
        rels_df,
        num_partitions,
    ):
        super().__init__(
            spark,
            output_dir,
            partitioned_evaluation,
            train_edges_df,
            valid_edges_df,
            test_edges_df,
            nodes_df,
            rels_df,
            num_partitions,
        )

        self.s3_bucket = "s3a://{}".format(os.getenv("S3_BUCKET"))
        self.s3 = s3fs.S3FileSystem(key=os.getenv("AWS_ACCESS_KEY_ID"), secret=os.getenv("AWS_SECRET_ACCESS_KEY"))
        self.nodes_dir = "{}/{}".format(self.s3_bucket, PathConstants.nodes_directory)
        self.edges_dir = "{}/{}".format(self.s3_bucket, PathConstants.edges_directory)
        self.dataset_stats_path = "{}/dataset.yaml".format(self.s3_bucket)
        self.s3.mkdir(self.nodes_dir)
        self.s3.mkdir(self.edges_dir)
        for mode in ["train", "valid", "test"]:
            self.s3.mkdir("{}{}".format(self.edges_dir, mode))
            self.s3.mkdir("{}{}_counts".format(self.edges_dir, mode))

        self.node_mapping_file = "{}{}".format(self.nodes_dir, PathConstants.node_mapping_file)
        self.relation_mapping_file = "{}{}".format(self.edges_dir, PathConstants.relation_mapping_file)
        self.edges_helper = {
            "train": [self.train_edges_df, "{}{}".format(self.edges_dir, PathConstants.train_edges_path)],
            "valid": [self.valid_edges_df, "{}{}".format(self.edges_dir, PathConstants.valid_edges_path)],
            "test": [self.test_edges_df, "{}{}".format(self.edges_dir, PathConstants.test_edges_path)],
        }
        self.offset_helper = {
            "train": "{}train/{}{}".format(
                self.edges_dir, PathConstants.training_file_prefix, PathConstants.partition_offsets_file
            ),
            "valid": "{}valid/{}{}".format(
                self.edges_dir, PathConstants.validation_file_prefix, PathConstants.partition_offsets_file
            ),
            "test": "{}test/{}{}".format(
                self.edges_dir, PathConstants.test_file_prefix, PathConstants.partition_offsets_file
            ),
        }

    def write_dataset_stats(self):
        dataset_stats = self.get_dataset_stats()
        dataset_stats.dataset_dir = "s3a://{}".format(self.s3_bucket)
        with self.s3.open(self.dataset_stats_path, "w") as f:
            yaml_file = OmegaConf.to_yaml(dataset_stats)
            f.writelines(yaml_file)
            f.flush()
        return dataset_stats

    def write_nodes_df(self):
        s3_write_df_to_csv(self.s3, self.nodes_df, self.nodes_dir, self.node_mapping_file)

    def write_rels_df(self):
        s3_write_df_to_csv(self.s3, self.rels_df, self.edges_dir, self.relation_mapping_file)

    def write_edges_df(self, mode):
        s3_write_df_to_csv(self.s3, self.edges_helper[mode][0], self.edges_dir + mode, self.edges_helper[mode][1])

    def write_partitioned_df_to_csv(self, mode):
        return s3_write_partitioned_df_to_csv(
            self.s3, self.edges_helper[mode][0], self.num_partitions, self.edges_dir + mode, self.edges_helper[mode][1]
        )

    def write_edge_offsets(self, mode, offsets):
        with self.s3.open(self.offset_helper[mode], "w") as f:
            f.writelines([str(o) + "\n" for o in offsets])
            f.flush()


class SparkWriter(object):
    def __init__(self, spark, output_dir, output_to_s3, partitioned_evaluation):
        self.spark = spark
        self.output_dir = output_dir
        self.output_to_s3 = output_to_s3
        if self.output_to_s3:
            self.s3 = s3fs.S3FileSystem(key=os.getenv("AWS_ACCESS_KEY_ID"), secret=os.getenv("AWS_SECRET_ACCESS_KEY"))
        self.partitioned_evaluation = partitioned_evaluation

    def write_to_csv(self, train_edges_df, valid_edges_df, test_edges_df, nodes_df, rels_df, num_partitions):
        writer = None
        if self.output_to_s3:
            writer = SparkWriteS3(
                self.spark,
                self.output_dir,
                self.partitioned_evaluation,
                train_edges_df,
                valid_edges_df,
                test_edges_df,
                nodes_df,
                rels_df,
                num_partitions,
            )
        else:
            writer = SparkWriterLocal(
                self.spark,
                self.output_dir,
                self.partitioned_evaluation,
                train_edges_df,
                valid_edges_df,
                test_edges_df,
                nodes_df,
                rels_df,
                num_partitions,
            )

        dataset_stats = writer.write_dataset_stats()
        writer.write_nodes_df()
        writer.write_rels_df()

        if writer.num_partitions > 1:
            offsets = writer.write_partitioned_df_to_csv("train")
            writer.write_edge_offsets("train", offsets)

            for mode in ["valid", "test"]:
                if writer.partitioned_evaluation:
                    offsets = writer.write_partitioned_df_to_csv(mode)
                    writer.write_edge_offsets(mode, offsets)
                else:
                    writer.write_edges_df(mode)
        else:
            for mode in ["train", "valid", "test"]:
                writer.write_edges_df(mode)

        return dataset_stats

    def write_to_binary(self, train_edges_df, valid_edges_df, test_edges_df, nodes_df, rels_df, num_partitions):
        dataset_stats = self.write_to_csv(
            train_edges_df, valid_edges_df, test_edges_df, nodes_df, rels_df, num_partitions
        )

        if self.output_to_s3:
            # TODO: merge csvs generated above and convert them to binary
            print("Binary mode output not yet supported for S3")
            exit()

        train_file = self.output_dir / Path(PathConstants.train_edges_path)
        valid_file = self.output_dir / Path(PathConstants.valid_edges_path)
        test_file = self.output_dir / Path(PathConstants.test_edges_path)

        tmp_train_file = TMP_DATA_DIRECTORY + "tmp_train_edges.tmp"
        tmp_valid_file = TMP_DATA_DIRECTORY + "tmp_valid_edges.tmp"
        tmp_test_file = TMP_DATA_DIRECTORY + "tmp_test_edges.tmp"

        print("Converting to binary")
        os.rename(train_file, tmp_train_file)
        convert_to_binary(tmp_train_file, train_file)

        if valid_edges_df is not None:
            os.rename(valid_file, tmp_valid_file)
            convert_to_binary(tmp_valid_file, valid_file)

        if test_edges_df is not None:
            os.rename(test_file, tmp_test_file)
            convert_to_binary(tmp_test_file, test_file)

        return dataset_stats
